{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install \"gymnasium[accept-rom-license]\"\n!pip -q install \"gymnasium[atari]\"","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:06:24.991041Z","iopub.execute_input":"2024-10-06T09:06:24.991495Z","iopub.status.idle":"2024-10-06T09:06:49.963148Z","shell.execute_reply.started":"2024-10-06T09:06:24.991453Z","shell.execute_reply":"2024-10-06T09:06:49.961691Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:06:49.966004Z","iopub.execute_input":"2024-10-06T09:06:49.966873Z","iopub.status.idle":"2024-10-06T09:06:51.128237Z","shell.execute_reply.started":"2024-10-06T09:06:49.966823Z","shell.execute_reply":"2024-10-06T09:06:51.127139Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Sun Oct  6 09:06:50 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   37C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.distributions import Categorical\nimport numpy as np\nimport gymnasium as gym\nfrom gymnasium.wrappers import GrayScaleObservation, FrameStack\nimport wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-06T09:06:51.129804Z","iopub.execute_input":"2024-10-06T09:06:51.130140Z","iopub.status.idle":"2024-10-06T09:06:51.136327Z","shell.execute_reply.started":"2024-10-06T09:06:51.130106Z","shell.execute_reply":"2024-10-06T09:06:51.135151Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Get ROM\nfrom ale_py import ALEInterface\nale = ALEInterface()\n\nfrom ale_py.roms import BattleZone\nale.loadROM(BattleZone)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:06:51.139705Z","iopub.execute_input":"2024-10-06T09:06:51.140221Z","iopub.status.idle":"2024-10-06T09:06:51.342032Z","shell.execute_reply.started":"2024-10-06T09:06:51.140143Z","shell.execute_reply":"2024-10-06T09:06:51.340953Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n[Powered by Stella]\nGame console created:\n  ROM file:  /opt/conda/lib/python3.10/site-packages/AutoROM/roms/battle_zone.bin\n  Cart Name: Battlezone (1983) (Atari) [!]\n  Cart MD5:  41f252a66c6301f1e8ab3612c19bc5d4\n  Display Format:  AUTO-DETECT ==> NTSC\n  ROM Size:        8192\n  Bankswitch Type: AUTO-DETECT ==> F8\n\nRunning ROM file...\nRandom seed is 1728205611\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nwandb_api = user_secrets.get_secret(\"WANDB_API_KEY\") ","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:06:51.343380Z","iopub.execute_input":"2024-10-06T09:06:51.343765Z","iopub.status.idle":"2024-10-06T09:06:51.486633Z","shell.execute_reply.started":"2024-10-06T09:06:51.343729Z","shell.execute_reply":"2024-10-06T09:06:51.485826Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class PPOPolicy(nn.Module):\n    def __init__(self, env):\n        super().__init__()\n\n        self.roi1_conv1 = nn.Conv2d(4, 32, 8, stride=4)\n        self.roi1_conv2 = nn.Conv2d(32, 64, 4, stride=2)\n        self.roi1_conv3 = nn.Conv2d(64, 64, 4, stride=2)\n\n        self.roi2_conv1 = nn.Conv2d(4, 16, 4, stride=2)\n        self.roi2_conv2 = nn.Conv2d(16, 32, 4, stride=2)\n\n        self.linear1 = nn.Linear(1792, 512)\n        self.actor = nn.Linear(512, 18)  # env.action_space\n        self.critic = nn.Linear(512, 1)  # Value output\n\n    def forward(self, x):\n        x = x / 255\n        x1, x2 = self.ROI(x)\n\n        # process ROI1\n        x1 = F.relu(self.roi1_conv1(x1))\n        x1 = F.relu(self.roi1_conv2(x1))\n        x1 = F.relu(self.roi1_conv3(x1))\n        x1 = x1.view(x1.shape[0], -1)\n\n        # process ROI2\n        x2 = F.relu(self.roi2_conv1(x2))\n        x2 = F.relu(self.roi2_conv2(x2))\n        x2 = x2.view(x2.shape[0], -1)\n\n        x3 = torch.cat((x1, x2), dim=1)\n        x3 = F.relu(self.linear1(x3))\n        return x3\n\n    def get_action(self, x):\n        x3 = self.forward(x)\n        logits = self.actor(x3)\n        probs = F.softmax(logits, dim=1)\n        return probs\n\n    def get_value(self, x):\n        x3 = self.forward(x)\n        value = self.critic(x3)\n        return value\n\n    def ROI(self, img):\n        \"\"\"\n        :return: roi1 - contains horizontal zone with tanks\n                 roi2 - contains radar\n        \"\"\"\n        height, width = img.shape[2], img.shape[3]\n\n        roi1 = img[:, :, int(height * 0.4) : int(height * 0.75), :]\n        roi2 = img[\n            :,\n            :,\n            int(height * 0.02) : int(height * 0.17),\n            int(width * 0.465) : int(width * 0.6),\n        ]\n\n        return roi1, roi2","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:06:51.487768Z","iopub.execute_input":"2024-10-06T09:06:51.488076Z","iopub.status.idle":"2024-10-06T09:06:51.502814Z","shell.execute_reply.started":"2024-10-06T09:06:51.488042Z","shell.execute_reply":"2024-10-06T09:06:51.502022Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def make_env(env_id, seed):\n    env = gym.make(env_id)\n    env = GrayScaleObservation(env)\n    env = FrameStack(env, 4)\n    env.action_space.seed(seed)\n    return env\n\n\ndef train(\n    env,\n    policy,\n    optimizer,\n    num_episodes=1000,\n    gamma=0.99,\n    clip_epsilon=0.2,\n    ppo_epochs=4,\n    wandb_name=wandb.util.generate_id()\n):  \n    wandb.init(\n        project=\"ppo-training-battlezone-rl\", \n        config={\n            \"num_episodes\": num_episodes,\n            \"gamma\": gamma,\n            \"clip_epsilon\": clip_epsilon,\n            \"ppo_epochs\": ppo_epochs,\n            \"learning_rate\": optimizer.param_groups[0]['lr'],\n        },\n        name=wandb_name\n    )\n    policy.train()\n    for episode in range(num_episodes):\n        obs, _ = env.reset()\n        log_probs = []\n        values = []\n        rewards = []\n        dones = []\n        states = []\n\n        done = False\n        total_reward = 0\n        while not done:\n            obs_tensor = torch.tensor(np.asarray(obs)).unsqueeze(0).float()\n            states.append(obs_tensor)\n\n            # Get action and value\n            probs = policy.get_action(obs_tensor)\n            value = policy.get_value(obs_tensor)\n            cat = Categorical(probs)\n            action = cat.sample()\n\n            # Step in the environment\n            obs, reward, done, _, _ = env.step(action.item())\n            total_reward += reward\n\n            # Store log probability, value, and reward\n            log_prob = cat.log_prob(action)\n            log_probs.append(log_prob)\n            values.append(value)\n            rewards.append(reward)\n            dones.append(done)\n\n        # Compute returns and advantages\n        returns = []\n        advantages = []\n        G = 0\n        for reward in reversed(rewards):\n            G = reward + gamma * G\n            returns.insert(0, G)\n        returns = torch.tensor(returns, dtype=torch.float32)\n        values = torch.cat(values).squeeze()\n        advantages = returns - values.detach()\n\n        # Normalize advantages\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-9)\n\n        # PPO Update\n        loss = None  # Initialize loss to prevent unbound variable error\n        log_probs = torch.cat(\n            log_probs\n        )  # Concatenate log_probs outside of loop to avoid in-place modifications\n        values = values.squeeze()  # Make sure values are properly squeezed\n\n        for _ in range(ppo_epochs):\n            new_log_probs = []\n            new_values = []\n            for state in states:\n                probs = policy.get_action(state)\n                value = policy.get_value(state)\n                cat = Categorical(probs)\n                action = cat.sample()\n                new_log_prob = cat.log_prob(action)\n                new_log_probs.append(new_log_prob)\n                new_values.append(value)\n\n            new_log_probs = torch.cat(new_log_probs)\n            new_values = torch.cat(new_values).squeeze()\n            ratios = torch.exp(\n                new_log_probs - log_probs.detach()\n            )  # Detach to avoid in-place modifications\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n            actor_loss = -torch.min(surr1, surr2).mean()\n            critic_loss = F.mse_loss(new_values, returns)\n            loss = actor_loss + 0.5 * critic_loss\n\n            # Update policy\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Log episode results to wandb\n        if loss is not None:\n            wandb.log({\n                \"episode\": episode + 1,\n                \"loss\": loss.item(),\n                \"total_reward\": total_reward\n            })\n            print(f\"Episode {episode + 1}/{num_episodes}, Loss: {loss.item()}, Total Reward: {total_reward}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:06:51.504357Z","iopub.execute_input":"2024-10-06T09:06:51.504685Z","iopub.status.idle":"2024-10-06T09:06:51.525687Z","shell.execute_reply.started":"2024-10-06T09:06:51.504629Z","shell.execute_reply":"2024-10-06T09:06:51.524711Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Initialize wandb\nwandb.login(key=wandb_api)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:06:51.526826Z","iopub.execute_input":"2024-10-06T09:06:51.527168Z","iopub.status.idle":"2024-10-06T09:06:51.598770Z","shell.execute_reply.started":"2024-10-06T09:06:51.527121Z","shell.execute_reply":"2024-10-06T09:06:51.597809Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melskow\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"env = make_env(\"ALE/BattleZone-v5\", 0)\npolicy = PPOPolicy(env)\noptimizer = Adam(policy.parameters(), lr=1e-4)\n\n# Train the policy\ntrain(env, policy, optimizer)\n\n# Save the trained model\ntorch.save(policy.state_dict(), \"trained_ppo_policy.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T09:06:51.600043Z","iopub.execute_input":"2024-10-06T09:06:51.600378Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n[Powered by Stella]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113773366666161, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43814ae89cec4efaa2436aec3041f619"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241006_090653-mkoc62m6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/elskow/ppo-training-battlezone-rl/runs/mkoc62m6' target=\"_blank\">osu5u7cm</a></strong> to <a href='https://wandb.ai/elskow/ppo-training-battlezone-rl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/elskow/ppo-training-battlezone-rl' target=\"_blank\">https://wandb.ai/elskow/ppo-training-battlezone-rl</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/elskow/ppo-training-battlezone-rl/runs/mkoc62m6' target=\"_blank\">https://wandb.ai/elskow/ppo-training-battlezone-rl/runs/mkoc62m6</a>"},"metadata":{}},{"name":"stdout","text":"Episode 1/1000, Loss: 62277.3515625, Total Reward: 3000.0\nEpisode 2/1000, Loss: 93489.296875, Total Reward: 3000.0\nEpisode 3/1000, Loss: 22033.025390625, Total Reward: 1000.0\nEpisode 4/1000, Loss: 22119.015625, Total Reward: 1000.0\nEpisode 5/1000, Loss: 4.024988174438477, Total Reward: 0.0\nEpisode 6/1000, Loss: 55883.03515625, Total Reward: 3000.0\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.save(\"/kaggle/working/trained_ppo_policy.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}